# %%
pip install pyautogen

# %%
pip install -qqq matplotlib numpy

# %%
import tempfile

from autogen import ConversableAgent
from autogen.coding import LocalCommandLineCodeExecutor

# Create a temporary directory to store the code files.
temp_dir = tempfile.TemporaryDirectory()

# Create a local command line code executor.
executor = LocalCommandLineCodeExecutor(
    timeout=10,  # Timeout for each code execution in seconds.
    work_dir="/kaggle/working/"  # Use the temporary directory to store the code files.
)

# Create an agent with code executor configuration.
code_executor_agent = ConversableAgent(
    "code_executor_agent",
    llm_config=False,  # Turn off LLM for this agent.
    code_execution_config={"executor": executor},  # Use the local command line code executor.
    human_input_mode="NEVER",  # Always take human input for this agent for safety.
)

# %%
message_with_code_block = """This is a message with code block.
The code block is below:
```python
import numpy as np
import matplotlib.pyplot as plt
x = np.random.randint(0, 100, 100)
y = np.random.randint(0, 100, 100)
plt.scatter(x, y)
plt.savefig('scatter.png')
print('Scatter plot saved to scatter.png')
```
This is the end of the message.
"""

# Generate a reply for the given code.
reply = code_executor_agent.generate_reply(messages=[{"role": "user", "content": message_with_code_block}])
print(reply)

# %%
import os

print(os.listdir(temp_dir.name))
# We can see the output scatter.png and the code file generated by the agent.

# %%
['scatter.png', '6507ea07b63b45aabb027ade4e213de6.py']

# %% [markdown]
# # # DockerCommandLineExecuter
# # 

# %%
from autogen.coding import DockerCommandLineCodeExecutor


# %%
from autogen.coding import DockerCommandLineCodeExecutor

# Create a temporary directory to store the code files.
# temp_dir = tempfile.TemporaryDirectory()

# Create a Docker command line code executor.
executor = DockerCommandLineCodeExecutor(
#     image="python:3.12-slim",  # Execute code using the given docker image name.
    timeout=10,  # Timeout for each code execution in seconds.
#     work_dir="/kaggle/working/"   # Use the temporary directory to store the code files.
)

# # Create an agent with code executor configuration that uses docker.
code_executor_agent_using_docker = ConversableAgent(
    "code_executor_agent_docker",
    llm_config=False,  # Turn off LLM for this agent.
    code_execution_config={"executor": executor},  # Use the docker command line code executor.
    human_input_mode="ALWAYS",  # Always take human input for this agent for safety.
)

# When the code executor is no longer used, stop it to release the resources.
# executor.stop()

# %% [markdown]
# # Use Code Execution in Conversation
# # 

# %%
# The code writer agent's system message is to instruct the LLM on how to use
# the code executor in the code executor agent.
code_writer_system_message = """You are a helpful AI assistant.
Solve tasks using your coding and language skills.
In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.
1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.
Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.
If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.
Reply 'TERMINATE' in the end when everything is done.
"""

code_writer_agent = ConversableAgent(
    "code_writer_agent",
    system_message=code_writer_system_message,
    llm_config={"config_list": [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]},
    code_execution_config=False,  # Turn off code execution for this agent.
)

# %%
chat_result = code_executor_agent.initiate_chat(
    code_writer_agent,
    message="Write Python code to calculate the 14th Fibonacci number.",
)

# %% [markdown]
# # Custom Tools Autogen

# %%
from typing import Annotated, Literal

Operator = Literal["+", "-", "*", "/"]


def calculator(a: int, b: int, operator: Annotated[Operator, "operator"]) -> int:
    if operator == "+":
        return a + b
    elif operator == "-":
        return a - b
    elif operator == "*":
        return a * b
    elif operator == "/":
        return int(a / b)
    else:
        raise ValueError("Invalid operator")

# %%
import os

from autogen import ConversableAgent

# Let's first define the assistant agent that suggests tool calls.
assistant = ConversableAgent(
    name="Assistant",
    system_message="You are a helpful AI assistant. "
    "You can help with simple calculations. "
    "Return 'TERMINATE' when the task is done.",
    llm_config={"config_list": [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]},
)

# The user proxy agent is used for interacting with the assistant agent
# and executes tool calls.
user_proxy = ConversableAgent(
    name="User",
    llm_config=False,
    is_termination_msg=lambda msg: msg.get("content") is not None and "TERMINATE" in msg["content"],
    human_input_mode="NEVER",
)

# Register the tool signature with the assistant agent.
assistant.register_for_llm(name="calculator", description="A simple calculator")(calculator)

# Register the tool function with the user proxy agent.
user_proxy.register_for_execution(name="calculator")(calculator)

# %%
from autogen import register_function

# Register the calculator function to the two agents.
register_function(
    calculator,
    caller=assistant,  # The assistant agent can suggest calls to the calculator.
    executor=user_proxy,  # The user proxy agent can execute the calculator calls.
    name="calculator",  # By default, the function name is used as the tool name.
    description="A simple calculator",  # A description of the tool.
)

# %%
chat_result = user_proxy.initiate_chat(assistant, message="What is (44232 + 13312 / (232 - 32)) * 5?")

# %% [markdown]
# # Conversable agent chat between two agent

# %%
import os

from autogen import ConversableAgent

student_agent = ConversableAgent(
    name="Student_Agent",
    system_message="""You are a student willing to learn.
    Understand the topic explained by teacher and ask doubt.
    Ask your teacher to be more clear if you do not understand in one go
    
    """,
llm_config={"config_list": [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]})
teacher_agent = ConversableAgent(
    name="Teacher_Agent",
    system_message="You are a math teacher.",
llm_config={"config_list": [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]})

chat_result = student_agent.initiate_chat(
    teacher_agent,
    message="Explain me all three netwons laws of motion",
    summary_method="reflection_with_llm",
    max_turns=5,
)

# %% [markdown]
# # Explooring Agents

# %%
! pip install -qqq pyautogen apify-client

# %%
import os

# config_list = [
#     {"model": "gpt-4", "api_key": os.getenv("OPENAI_API_KEY")},
# ]

llm_config= [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]

apify_api_key = ""

# %%
from apify_client import ApifyClient
from typing_extensions import Annotated


def scrape_page(url: Annotated[str, "The URL of the web page to scrape"]) -> Annotated[str, "Scraped content"]:
    # Initialize the ApifyClient with your API token
    client = ApifyClient(token=apify_api_key)

    # Prepare the Actor input
    run_input = {
        "startUrls": [{"url": url}],
        "useSitemaps": False,
        "crawlerType": "playwright:firefox",
        "includeUrlGlobs": [],
        "excludeUrlGlobs": [],
        "ignoreCanonicalUrl": False,
        "maxCrawlDepth": 0,
        "maxCrawlPages": 1,
        "initialConcurrency": 0,
        "maxConcurrency": 200,
        "initialCookies": [],
        "proxyConfiguration": {"useApifyProxy": True},
        "maxSessionRotations": 10,
        "maxRequestRetries": 5,
        "requestTimeoutSecs": 60,
        "dynamicContentWaitSecs": 10,
        "maxScrollHeightPixels": 5000,
        "removeElementsCssSelector": """nav, footer, script, style, noscript, svg,
    [role=\"alert\"],
    [role=\"banner\"],
    [role=\"dialog\"],
    [role=\"alertdialog\"],
    [role=\"region\"][aria-label*=\"skip\" i],
    [aria-modal=\"true\"]""",
        "removeCookieWarnings": True,
        "clickElementsCssSelector": '[aria-expanded="false"]',
        "htmlTransformer": "readableText",
        "readableTextCharThreshold": 100,
        "aggressivePrune": False,
        "debugMode": True,
        "debugLog": True,
        "saveHtml": True,
        "saveMarkdown": True,
        "saveFiles": False,
        "saveScreenshots": False,
        "maxResults": 9999999,
        "clientSideMinChangePercentage": 15,
        "renderingTypeDetectionPercentage": 10,
    }

    # Run the Actor and wait for it to finish
    run = client.actor("aYG0l9s7dbB7j3gbS").call(run_input=run_input)

    # Fetch and print Actor results from the run's dataset (if there are any)
    text_data = ""
    for item in client.dataset(run["defaultDatasetId"]).iterate_items():
        text_data += item.get("text", "") + "\n"

    average_token = 0.75
    max_tokens = 20000  # slightly less than max to be safe 32k
    text_data = text_data[: int(average_token * max_tokens)]
    return text_data

# %%
from autogen import ConversableAgent, register_function

# Create web scrapper agent.
scraper_agent = ConversableAgent(
    "WebScraper",
    llm_config={"config_list": llm_config},
    system_message="You are a web scrapper and you can scrape any web page using the tools provided. "
    "Returns 'TERMINATE' when the scraping is done.",
)

# Create user proxy agent.
user_proxy_agent = ConversableAgent(
    "UserProxy",
    llm_config=False,  # No LLM for this agent.
    human_input_mode="Always",
    code_execution_config=False,  # No code execution for this agent.
    is_termination_msg=lambda x: x.get("content", "") is not None and "terminate" in x["content"].lower(),
    default_auto_reply="Please continue if not finished, otherwise return 'TERMINATE'.",
)

# Register the function with the agents.
register_function(
    scrape_page,
    caller=scraper_agent,
    executor=user_proxy_agent,
    name="scrape_page",
    description="Scrape a web page and return the content.",
)

# %%
chat_result = user_proxy_agent.initiate_chat(
    scraper_agent,
    message="Can you scrape medium.com for me?",
    max_turns = 4,
    summary_method="reflection_with_llm",
    summary_args={
        "summary_prompt": """Summarize the scraped content and format summary EXACTLY as follows:
---
*Company name*:
`name of company`
---
*Website*:
`website of company`
---
*Description*:
`Company that does things.`
---
*Tags*:
`Manufacturing. Retail. E-commerce.`
---
*Takeaways*:
`Provides shareholders with value by selling products.`
---
*Questions*:
`What products do they sell? How do they make money? What is their market share?`
---
"""
    },
)

# %%
print(chat_result.summary)

# %%
pip install pyautogen

# %%
llm_config= [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]

# %%
import autogen
from autogen import ConversableAgent, UserProxyAgent

# %%
summrizer_agent  =  ConversableAgent(
    "summrizer",
    llm_config={'config_list':llm_config},
    system_message="""
                    Your task is to provide top 5 topics grom given text.
                    The length of topics should be not more than 5 words.                 
    """

)

student_agent = ConversableAgent(
    "student_agent",
system_message = "you have to ask for summarization from summrizer agent",
    llm_config = False,
 human_input_mode="Always",
    code_execution_config=False,  # No code execution for this agent.
    is_termination_msg=lambda x: x.get("content", "") is not None and "terminate" in x["content"].lower(),
    default_auto_reply="Please continue if not finished, otherwise return 'TERMINATE'.",
)

# %%
text = """
Football, often referred to as soccer in some parts of the world, is a sport that captivates the hearts and minds of millions globally. Played on a rectangular field between two teams, each comprising eleven players, football's simplicity belies its profound impact. The objective is straightforward: score goals by maneuvering a spherical ball into the opponent's net, without using hands or arms except for the goalkeeper within the penalty area. 
Originating in England in the 19th century, football quickly spread across continents, transcending cultural, social, and geographical boundaries. Its popularity burgeoned, becoming the world's most widely followed and participated sport. Whether in the dusty streets of Rio de Janeiro or the meticulously manicured pitches of Europe's elite clubs, football's essence remains unchanged – a beautiful game that unites individuals and communities alike.
Beyond the exhilarating matches and dazzling displays of skill, football embodies profound values. It fosters teamwork, discipline, resilience, and sportsmanship. From grassroots academies nurturing the stars of tomorrow to the pinnacle of global competitions like the FIFA World Cup, football inspires millions to strive for excellence and unity.
The allure of football lies not only in its athletic prowess but also in its ability to forge connections and transcend barriers. It serves as a universal language, transcending linguistic and cultural disparities, enabling people from diverse backgrounds to bond over a shared passion. Whether cheering for their favorite teams in packed stadiums or watching matches from afar, football fans experience a sense of belonging and camaraderie.
Moreover, football has become a platform for social change and activism. Players, clubs, and organizations use their influence to address societal issues such as racism, inequality, and discrimination. Through initiatives promoting inclusivity, diversity, and community engagement, football continues to strive for a more equitable and compassionate world.
In essence, football is more than just a sport; it is a global phenomenon that enriches lives, builds bridges, and inspires generations. Its enduring appeal lies in its ability to transcend borders and unite people from all walks of life under the banner of the beautiful game. As long as there are players kicking a ball and fans cheering from the stands, football's legacy will endure as a testament to the power of sport to unite and inspire.
"""

# %%
result = student_agent.initiate_chat(summrizer_agent, message= f'give me topics of {text}' , max_turns=2)

# %% [markdown]
# # Groupchat

# %%
task = "Write an article about thought process of Vincent Van goh and about his paintings"

# %%
writer = autogen.AssistantAgent(
    name="Writer",
    llm_config={"config_list": llm_config},
    system_message="""
    You are a professional writer, known for your insightful and engaging articles.
    You transform complex concepts into compelling narratives.
    You should imporve the quality of the content based on the feedback from the user.
    """,
)

user_proxy = autogen.UserProxyAgent(
    name="User",
    human_input_mode="NEVER",
    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,
    code_execution_config={
        "last_n_messages": 1,
        "work_dir": "tasks",
        "use_docker": False,
    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.
)

critic = autogen.AssistantAgent(
    name="Critic",
    llm_config={"config_list": llm_config},
    system_message="""
    You are a critic, known for your thoroughness and commitment to standards.
    Your task is to scrutinize content for any harmful elements or regulatory violations, ensuring
    all materials align with required guidelines.
    For code
    """,
)

# %%
def reflection_message(recipient, messages, sender, config):
    print("Reflecting...", "yellow")
    return f"Reflect and provide critique on the following writing. \n\n {recipient.chat_messages_for_summary(sender)[-1]['content']}"


user_proxy.register_nested_chats(
    [{"recipient": critic, "message": reflection_message, "summary_method": "last_msg", "max_turns": 1}],
    trigger=writer,  # condition=my_condition,
)

res = user_proxy.initiate_chat(recipient=writer, message=task, max_turns=2, summary_method="last_msg")

# %% [markdown]
# # Group Chat

# %%
llm_configs = {"config_list": llm_config, "cache_seed": 42}
user_proxy = autogen.UserProxyAgent(
    name="User_proxy",
    system_message="A human admin.",
    code_execution_config={
        "last_n_messages": 2,
        "work_dir": "groupchat",
        "use_docker": False,
    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.
    human_input_mode="TERMINATE",
)
coder = autogen.AssistantAgent(
    name="Coder",
    llm_config=llm_configs,
)
pm = autogen.AssistantAgent(
    name="Product_manager",
    system_message="Creative in software product ideas.",
    llm_config=llm_configs,
)
groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_configs)

# %%
user_proxy.initiate_chat(
    manager, message="Find a latest paper about gpt-4 on arxiv and find its potential applications in software."
)

# %%
import os

from autogen import ConversableAgent

agent = ConversableAgent(
    "chatbot",
    llm_config={"config_list":[
  {
    "model": "",
    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""

  }
]},
    code_execution_config=False,  # Turn off code execution, by default it is off.
    function_map=None,  # No registered functions, by default it is None.
    human_input_mode="NEVER",  # Never ask for human input.
)

# %%
cathy = ConversableAgent(
    "cathy",
    system_message="Your name is Cathy and you are a part of a duo of comedians.,
    llm_config={"config_list": [
  {
     "model" :"aiml-interns-gpt35-turbo", # model = "deployment_name"

    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]},
    human_input_mode="NEVER",  # Never ask for human input.
)

joe = ConversableAgent(
    "joe",
    system_message="Your name is Joe and you are a part of a duo of comedians.",
    llm_config={"config_list": [
  {
    "model": "",
    "api_type": "azure",
    "api_key": "",
    "base_url": "",
    "api_version":""
  }
]},
    human_input_mode="NEVER",  # Never ask for human input.
)

# %%
result = joe.initiate_chat(cathy, message="Cathy, tell me a joke.", max_turns=2)

# %%
pip install crewai

# %%
pip install 'crewai[tools]'

# %%


# %%
pip install langchain

# %%
pip install langchain_openai

# %%
from langchain_openai import AzureChatOpenAI

# %%
azure_llm = AzureChatOpenAI(
  api_version="2024-02-15-preview",
  model="aiml-interns-gpt35-turbo", # model = "deployment_name"

  azure_endpoint = "", 

  api_key=""
)

# %%
# llm = azure_llm()

# %%
import os


# OPENAI_API_KEY = "e79961ea273547288d3c777034ae724e"
# model= "aiml-interns-gpt35-turbo",
# #     api_type": "azure",
# #     "api_key: "e79961ea273547288d3c777034ae724e",
# base_url= "https://aiml-interns.openai.azure.com/",
# api_version="2024-02-15-preview"
SERPER_API_KEY = "" # serper.dev API key


import os
from crewai import Agent, Task, Crew, Process
from crewai_tools import SerperDevTool

# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"
# os.environ["SERPER_API_KEY"] = "Your Key" # serper.dev API key

# You can choose to use a local model through Ollama for example. See https://docs.crewai.com/how-to/LLM-Connections/ for more information.

# os.environ["OPENAI_API_BASE"] = 'http://localhost:11434/v1'
# os.environ["OPENAI_MODEL_NAME"] ='openhermes'  # Adjust based on available model
# os.environ["OPENAI_API_KEY"] ='sk-111111111111111111111111111111111111111111111111'

search_tool = SerperDevTool()

# Define your agents with roles and goals
researcher = Agent(
  role='Senior Research Analyst',
  goal='Uncover cutting-edge developments in AI and data science',
  backstory="""You work at a leading tech think tank.
  Your expertise lies in identifying emerging trends.
  You have a knack for dissecting complex data and presenting actionable insights.""",
  verbose=True,
  allow_delegation=False,
  tools=[search_tool]
  # You can pass an optional llm attribute specifying what model you wanna use.
  # It can be a local model through Ollama / LM Studio or a remote
  # model like OpenAI, Mistral, Antrophic or others (https://docs.crewai.com/how-to/LLM-Connections/)
  #
  # import os
  # os.environ['OPENAI_MODEL_NAME'] = 'gpt-3.5-turbo'
  #
  # OR
  #
  # from langchain_openai import ChatOpenAI
  # llm=ChatOpenAI(model_name="gpt-3.5", temperature=0.7)
)
writer = Agent(
  role='Tech Content Strategist',
  goal='Craft compelling content on tech advancements',
  backstory="""You are a renowned Content Strategist, known for your insightful and engaging articles.
  You transform complex concepts into compelling narratives.""",
  verbose=True,
  allow_delegation=True,
  llm=azure_llm
)

# Create tasks for your agents
task1 = Task(
  description="""Conduct a comprehensive analysis of the latest advancements in AI in 2024.
  Identify key trends, breakthrough technologies, and potential industry impacts.""",
  expected_output="Full analysis report in bullet points",
  agent=researcher
)

task2 = Task(
  description="""Using the insights provided, develop an engaging blog
  post that highlights the most significant AI advancements.
  Your post should be informative yet accessible, catering to a tech-savvy audience.
  Make it sound cool, avoid complex words so it doesn't sound like AI.""",
  expected_output="Full blog post of at least 4 paragraphs",
  agent=writer
)

# Instantiate your crew with a sequential process
crew = Crew(
  agents=[researcher, writer],
  tasks=[task1, task2],
  verbose=2, # You can set it to 1 or 2 to different logging levels
)

# Get your crew to work!
result = crew.kickoff()

print("######################")
print(result)

# %%



